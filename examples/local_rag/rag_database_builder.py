"""
RAGæ•°æ®åº“æ„å»ºå™¨

ä½¿ç”¨æ–¹æ³•ï¼š
1. ä½¿ç”¨é»˜è®¤å‚æ•°ï¼š
   builder = RAGDatabaseBuilder()

2. è‡ªå®šä¹‰é…ç½®ï¼š
   builder = RAGDatabaseBuilder(
       documents_path="./docs",
       embedding_model="Qwen/Qwen3-Embedding-0.6B",
       chunk_size=1200
   )

3. ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°ï¼š
   python rag_database_builder.py --force-rebuild
   python rag_database_builder.py --embedding-model Qwen/Qwen3-Embedding-0.6B
"""

import os
import logging
from pathlib import Path
from typing import List, Optional
import hashlib



# æ–‡æ¡£åŠ è½½å™¨
from langchain_community.document_loaders import (
    TextLoader,
    PyPDFLoader,
    Docx2txtLoader,
    UnstructuredFileLoader,
    DirectoryLoader
)
from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings

# ç¦ç”¨åŒ¿åé¥æµ‹
os.environ.setdefault("ANONYMIZED_TELEMETRY", "False")

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class RAGDatabaseBuilder:
    """æœ¬åœ°RAGå‘é‡æ•°æ®åº“æ„å»ºå™¨"""
    
    def __init__(
        self,
        documents_path: str = "./my_documents",
        vector_db_path: str = "./local_vector_db", 
        embedding_model: str = "Qwen/Qwen3-Embedding-0.6B",
        chunk_size: int = 1000,
        chunk_overlap: int = 200,
        collection_name: str = "local_documents"
    ):
        self.documents_path = Path(documents_path)
        self.vector_db_path = Path(vector_db_path)
        self.embedding_model_name = embedding_model
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.collection_name = collection_name
        
        # æ”¯æŒçš„æ–‡ä»¶æ ¼å¼
        self.supported_extensions = {
            '.txt': TextLoader,
            '.pdf': PyPDFLoader,
            '.docx': Docx2txtLoader,
            '.doc': UnstructuredFileLoader,
            '.md': TextLoader,
            '.html': UnstructuredFileLoader,
            '.xml': UnstructuredFileLoader,
        }
        
        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        self.embeddings = None
        self._init_embeddings()
    
    def _init_embeddings(self):
        """åˆå§‹åŒ–åµŒå…¥æ¨¡å‹"""
        logger.info(f"åŠ è½½åµŒå…¥æ¨¡å‹: {self.embedding_model_name}")
        self.embeddings = HuggingFaceEmbeddings(
            model_name=self.embedding_model_name,
            model_kwargs={'device': 'cpu'},  # ä½¿ç”¨CPUï¼Œå¦‚æœæœ‰GPUå¯ä»¥æ”¹ä¸º'cuda'
            encode_kwargs={'normalize_embeddings': True}
        )
    
    def _get_file_hash(self, file_path: Path) -> str:
        """è·å–æ–‡ä»¶å“ˆå¸Œå€¼ï¼Œç”¨äºæ£€æµ‹æ–‡ä»¶å˜åŒ–"""
        with open(file_path, 'rb') as f:
            return hashlib.md5(f.read()).hexdigest()
    
    def _load_documents(self) -> List[Document]:
        """åŠ è½½æœ¬åœ°æ–‡æ¡£"""
        logger.info(f"ä» {self.documents_path} åŠ è½½æ–‡æ¡£...")
        
        if not self.documents_path.exists():
            raise ValueError(f"æ–‡æ¡£è·¯å¾„ä¸å­˜åœ¨: {self.documents_path}")
        
        all_docs = []
        file_count = 0
        
        # éå†æ–‡æ¡£ç›®å½•
        for file_path in self.documents_path.rglob("*"):
            if file_path.is_file() and file_path.suffix.lower() in self.supported_extensions:
                try:
                    docs = self._load_single_file(file_path)
                    all_docs.extend(docs)
                    file_count += 1
                    logger.info(f"å·²åŠ è½½: {file_path.name}")
                except Exception as e:
                    logger.error(f"åŠ è½½æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        
        logger.info(f"æ€»å…±åŠ è½½äº† {file_count} ä¸ªæ–‡ä»¶ï¼Œ{len(all_docs)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
        return all_docs
    
    def _load_single_file(self, file_path: Path) -> List[Document]:
        """åŠ è½½å•ä¸ªæ–‡ä»¶"""
        file_extension = file_path.suffix.lower()
        loader_class = self.supported_extensions[file_extension]
        
        try:
            loader = loader_class(str(file_path))
            docs = loader.load()
            
            # æ·»åŠ å…ƒæ•°æ®
            for doc in docs:
                doc.metadata.update({
                    'source': str(file_path),
                    'filename': file_path.name,
                    'file_type': file_extension,
                    'file_hash': self._get_file_hash(file_path)
                })
            
            return docs
        except Exception as e:
            logger.error(f"ä½¿ç”¨ {loader_class} åŠ è½½ {file_path} å¤±è´¥: {e}")
            # å°è¯•ä½¿ç”¨é€šç”¨åŠ è½½å™¨
            try:
                loader = UnstructuredFileLoader(str(file_path))
                docs = loader.load()
                for doc in docs:
                    doc.metadata.update({
                        'source': str(file_path),
                        'filename': file_path.name,
                        'file_type': file_extension,
                        'file_hash': self._get_file_hash(file_path)
                    })
                return docs
            except Exception as e2:
                logger.error(f"é€šç”¨åŠ è½½å™¨ä¹Ÿå¤±è´¥: {e2}")
                return []
    
    def _split_documents(self, docs: List[Document]) -> List[Document]:
        """åˆ†å‰²æ–‡æ¡£"""
        logger.info("åˆ†å‰²æ–‡æ¡£...")
        
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            length_function=len,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", ".", "!", "?", " ", ""],
            add_start_index=True,
        )
        
        # è¿‡æ»¤ç©ºæ–‡æ¡£
        valid_docs = [doc for doc in docs if doc.page_content.strip()]
        
        split_docs = text_splitter.split_documents(valid_docs)
        
        # å»é‡
        unique_docs = []
        seen_content = set()
        
        for doc in split_docs:
            content_hash = hashlib.md5(doc.page_content.encode()).hexdigest()
            if content_hash not in seen_content:
                seen_content.add(content_hash)
                unique_docs.append(doc)
        
        logger.info(f"åˆ†å‰²åå¾—åˆ° {len(split_docs)} ä¸ªæ–‡æ¡£ç‰‡æ®µï¼Œå»é‡å {len(unique_docs)} ä¸ª")
        return unique_docs
    
    def build_database(self, force_rebuild: bool = False) -> Chroma:
        """æ„å»ºå‘é‡æ•°æ®åº“"""
        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡å»º
        if not force_rebuild and self.database_exists():
            logger.info("å‘é‡æ•°æ®åº“å·²å­˜åœ¨ï¼Œå°è¯•åŠ è½½ç°æœ‰æ•°æ®åº“...")
            try:
                return self.load_database()
            except Exception as e:
                if "expecting embedding with dimension" in str(e):
                    logger.warning(f"âš ï¸  æ£€æµ‹åˆ°åµŒå…¥æ¨¡å‹ç»´åº¦ä¸åŒ¹é…: {e}")
                    logger.warning(f"å½“å‰æ¨¡å‹ {self.embedding_model_name} ä¸ç°æœ‰æ•°æ®åº“ä¸å…¼å®¹")
                    logger.info("ğŸ”„ è‡ªåŠ¨åˆ‡æ¢åˆ°é‡å»ºæ¨¡å¼...")
                    force_rebuild = True
                else:
                    raise e
        
        # å¦‚æœéœ€è¦é‡å»ºï¼Œå…ˆåˆ é™¤æ—§æ•°æ®åº“
        if force_rebuild and self.database_exists():
            logger.info("ğŸ—‘ï¸  åˆ é™¤ç°æœ‰æ•°æ®åº“...")
            self.delete_database()
        
        # åŠ è½½æ–‡æ¡£
        documents = self._load_documents()
        
        if not documents:
            raise ValueError("æ²¡æœ‰æ‰¾åˆ°å¯åŠ è½½çš„æ–‡æ¡£")
        
        # åˆ†å‰²æ–‡æ¡£
        split_docs = self._split_documents(documents)
        
        # åˆ›å»ºå‘é‡æ•°æ®åº“
        logger.info("åˆ›å»ºå‘é‡æ•°æ®åº“...")
        vector_store = Chroma.from_documents(
            documents=split_docs,
            embedding=self.embeddings,
            persist_directory=str(self.vector_db_path),
            collection_name=self.collection_name
        )
        
        logger.info(f"å‘é‡æ•°æ®åº“å·²ä¿å­˜åˆ°: {self.vector_db_path}")
        return vector_store
    
    def load_database(self) -> Chroma:
        """åŠ è½½ç°æœ‰å‘é‡æ•°æ®åº“"""
        if not self.database_exists():
            raise ValueError(f"å‘é‡æ•°æ®åº“ä¸å­˜åœ¨: {self.vector_db_path}")
        
        logger.info("åŠ è½½ç°æœ‰å‘é‡æ•°æ®åº“...")
        vector_store = Chroma(
            persist_directory=str(self.vector_db_path),
            embedding_function=self.embeddings,
            collection_name=self.collection_name
        )
        
        # æ£€æŸ¥æ•°æ®åº“æ˜¯å¦ä¸ºç©º
        try:
            collection = vector_store._collection
            count = collection.count()
            logger.info(f"å‘é‡æ•°æ®åº“åŒ…å« {count} ä¸ªæ–‡æ¡£")
            
            if count == 0:
                logger.warning("å‘é‡æ•°æ®åº“ä¸ºç©ºï¼Œéœ€è¦é‡æ–°æ„å»º")
                
        except Exception as e:
            logger.error(f"æ£€æŸ¥å‘é‡æ•°æ®åº“æ—¶å‡ºé”™: {e}")
        
        return vector_store
    
    def database_exists(self) -> bool:
        """æ£€æŸ¥å‘é‡æ•°æ®åº“æ˜¯å¦å­˜åœ¨"""
        return self.vector_db_path.exists() and len(list(self.vector_db_path.iterdir())) > 0
    
    def add_documents_to_database(self, new_docs_path: str, vector_store: Chroma = None):
        """æ·»åŠ æ–°æ–‡æ¡£åˆ°ç°æœ‰æ•°æ®åº“"""
        logger.info(f"æ·»åŠ æ–°æ–‡æ¡£ä»: {new_docs_path}")
        
        if vector_store is None:
            vector_store = self.load_database()
        
        # ä¸´æ—¶æ”¹å˜æ–‡æ¡£è·¯å¾„
        original_path = self.documents_path
        self.documents_path = Path(new_docs_path)
        
        try:
            # åŠ è½½æ–°æ–‡æ¡£
            new_documents = self._load_documents()
            if new_documents:
                # åˆ†å‰²æ–°æ–‡æ¡£
                split_docs = self._split_documents(new_documents)
                
                # æ·»åŠ åˆ°ç°æœ‰å‘é‡æ•°æ®åº“
                vector_store.add_documents(split_docs)
                logger.info(f"æˆåŠŸæ·»åŠ  {len(split_docs)} ä¸ªæ–°æ–‡æ¡£ç‰‡æ®µ")
        finally:
            # æ¢å¤åŸå§‹è·¯å¾„
            self.documents_path = original_path
    
    def get_database_info(self, vector_store: Chroma = None) -> dict:
        """è·å–æ•°æ®åº“ä¿¡æ¯"""
        if vector_store is None:
            if not self.database_exists():
                return {"error": "æ•°æ®åº“ä¸å­˜åœ¨"}
            vector_store = self.load_database()
        
        try:
            collection = vector_store._collection
            count = collection.count()
            
            return {
                "document_count": count,
                "embedding_model": self.embedding_model_name,
                "chunk_size": self.chunk_size,
                "chunk_overlap": self.chunk_overlap,
                "vector_db_path": str(self.vector_db_path),
                "documents_path": str(self.documents_path),
                "collection_name": self.collection_name
            }
        except Exception as e:
            return {"error": str(e)}
    
    def delete_database(self):
        """åˆ é™¤å‘é‡æ•°æ®åº“"""
        if self.vector_db_path.exists():
            import shutil
            shutil.rmtree(self.vector_db_path)
            logger.info(f"å·²åˆ é™¤å‘é‡æ•°æ®åº“: {self.vector_db_path}")

def build_rag_database(
    documents_path: str = "./my_documents",
    vector_db_path: str = "./local_vector_db",
    embedding_model: str = "Qwen/Qwen3-Embedding-0.6B",
    chunk_size: int = 1000,
    chunk_overlap: int = 200,
    force_rebuild: bool = False
) -> Chroma:
    """
    æ„å»ºRAGå‘é‡æ•°æ®åº“çš„ä¾¿æ·å‡½æ•°
    
    Args:
        documents_path: æ–‡æ¡£è·¯å¾„
        vector_db_path: å‘é‡æ•°æ®åº“ä¿å­˜è·¯å¾„
        embedding_model: åµŒå…¥æ¨¡å‹åç§°
        chunk_size: æ–‡æ¡£åˆ†å—å¤§å°
        chunk_overlap: åˆ†å—é‡å å¤§å°
        force_rebuild: æ˜¯å¦å¼ºåˆ¶é‡å»º
    
    Returns:
        Chroma: å‘é‡æ•°æ®åº“å®ä¾‹
    """
    builder = RAGDatabaseBuilder(
        documents_path=documents_path,
        vector_db_path=vector_db_path,
        embedding_model=embedding_model,
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap
    )
    
    return builder.build_database(force_rebuild=force_rebuild)

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(
        description="RAGå‘é‡æ•°æ®åº“æ„å»ºå™¨",
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python rag_database_builder.py                    # æ­£å¸¸æ„å»º/åŠ è½½æ•°æ®åº“
  python rag_database_builder.py --force-rebuild   # å¼ºåˆ¶é‡å»ºæ•°æ®åº“
  python rag_database_builder.py --info            # æ˜¾ç¤ºæ•°æ®åº“ä¿¡æ¯
        """,
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    parser.add_argument(
        "--force-rebuild",
        action="store_true",
        help="å¼ºåˆ¶é‡å»ºå‘é‡æ•°æ®åº“ï¼Œå³ä½¿å·²å­˜åœ¨"
    )
    
    parser.add_argument(
        "--info",
        action="store_true", 
        help="æ˜¾ç¤ºæ•°æ®åº“ä¿¡æ¯è€Œä¸æ„å»º"
    )
    
    parser.add_argument(
        "--documents-path",
        type=str,
        default="./my_documents",
        help="æ–‡æ¡£è·¯å¾„ (é»˜è®¤: ./my_documents)"
    )
    
    parser.add_argument(
        "--vector-db-path", 
        type=str,
        default="./local_vector_db",
        help="å‘é‡æ•°æ®åº“è·¯å¾„ (é»˜è®¤: ./local_vector_db)"
    )
    
    parser.add_argument(
        "--embedding-model",
        type=str,
        default="Qwen/Qwen3-Embedding-0.6B",
        help="åµŒå…¥æ¨¡å‹åç§° (é»˜è®¤: Qwen/Qwen3-Embedding-0.6B)"
    )
    
    args = parser.parse_args()
    
    if args.info:
        # æ˜¾ç¤ºæ•°æ®åº“ä¿¡æ¯
        try:
            builder = RAGDatabaseBuilder(
                documents_path=args.documents_path,
                vector_db_path=args.vector_db_path,
                embedding_model=args.embedding_model
            )
            
            if builder.database_exists():
                print("ğŸ“Š æ•°æ®åº“ä¿¡æ¯:")
                info = builder.get_database_info()
                for key, value in info.items():
                    print(f"   {key}: {value}")
            else:
                print("âŒ æ•°æ®åº“ä¸å­˜åœ¨")
        except Exception as e:
            print(f"âŒ è·å–æ•°æ®åº“ä¿¡æ¯å¤±è´¥: {e}")
    else:
        # æ„å»º/åŠ è½½æ•°æ®åº“
        print(f"ğŸ”¨ å¼€å§‹æ„å»ºRAGæ•°æ®åº“...")
        print(f"ğŸ“Š å¼ºåˆ¶é‡å»º: {args.force_rebuild}")
        print(f"ğŸ“ æ–‡æ¡£è·¯å¾„: {args.documents_path}")
        print(f"ğŸ’¾ æ•°æ®åº“è·¯å¾„: {args.vector_db_path}")
        print(f"ğŸ¤– åµŒå…¥æ¨¡å‹: {args.embedding_model}")
            
        vector_store = build_rag_database(
            documents_path=args.documents_path,
            vector_db_path=args.vector_db_path,
            embedding_model=args.embedding_model,
            force_rebuild=args.force_rebuild
        )
        
        print("âœ… æ•°æ®åº“æ„å»ºå®Œæˆï¼")