import logging
from typing import List
import os
import argparse

from langchain_chroma import Chroma
from smolagents import Tool, CodeAgent, LiteLLMModel

from rag_database_builder import build_rag_database

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

from dotenv import load_dotenv
load_dotenv(override=True)

class LocalRetrieverTool(Tool):
    """æœ¬åœ°æ–‡æ¡£æ£€ç´¢å·¥å…·"""
    
    name = "local_retriever"
    description = """
    ä½¿ç”¨è¯­ä¹‰æœç´¢ä»æœ¬åœ°æ–‡æ¡£åº“ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯ã€‚
    è¿™ä¸ªå·¥å…·å¯ä»¥æœç´¢æœ¬åœ°çš„txtã€pdfã€docxã€mdç­‰æ ¼å¼çš„æ–‡æ¡£ã€‚
    æ”¯æŒä¸­æ–‡å’Œè‹±æ–‡æ–‡æ¡£æ£€ç´¢ã€‚
    """
    
    inputs = {
        "query": {
            "type": "string",
            "description": "è¦æœç´¢çš„æŸ¥è¯¢å†…å®¹ã€‚åº”è¯¥ä½¿ç”¨é™ˆè¿°å¥è€Œä¸æ˜¯ç–‘é—®å¥ï¼Œä¾‹å¦‚ï¼š'æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒæ–¹æ³•' è€Œä¸æ˜¯ 'å¦‚ä½•è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹ï¼Ÿ'"
        },
        "top_k": {
            "type": "integer", 
            "description": "è¿”å›çš„ç›¸å…³æ–‡æ¡£æ•°é‡ï¼Œé»˜è®¤ä¸º5",
            "default": 5,
            "nullable": True
        }
    }
    output_type = "string"
    
    def __init__(self, vector_store: Chroma, **kwargs):
        super().__init__(**kwargs)
        self.vector_store = vector_store
    
    def forward(self, query: str, top_k: int = 5) -> str:
        """æ‰§è¡Œæ–‡æ¡£æ£€ç´¢"""
        try:
            # æ‰§è¡Œç›¸ä¼¼åº¦æœç´¢
            docs = self.vector_store.similarity_search_with_score(query, k=top_k)
            
            if not docs:
                return "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£ã€‚è¯·å°è¯•ä½¿ç”¨ä¸åŒçš„å…³é”®è¯æˆ–æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å·²æ­£ç¡®åŠ è½½ã€‚"
            
            # æ ¼å¼åŒ–ç»“æœ
            results = []
            for i, (doc, score) in enumerate(docs):
                # è·å–æ–‡æ¡£ä¿¡æ¯
                filename = doc.metadata.get('filename', 'æœªçŸ¥æ–‡ä»¶')
                file_type = doc.metadata.get('file_type', 'æœªçŸ¥')
                
                results.append(f"""
===== æ–‡æ¡£ {i+1} (ç›¸ä¼¼åº¦åˆ†æ•°: {score:.3f}) =====
æ¥æºæ–‡ä»¶: {filename} ({file_type})
å†…å®¹: {doc.page_content.strip()}
""")
            
            return "\næ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£:\n" + "\n".join(results)
            
        except Exception as e:
            error_msg = f"æ£€ç´¢è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}"
            logger.error(error_msg)
            return error_msg
    
    def get_database_stats(self) -> dict:
        """è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
        try:
            collection = self.vector_store._collection
            count = collection.count()
            return {
                "total_documents": count,
                "database_path": self.vector_store._persist_directory,
                "collection_name": self.vector_store._collection.name
            }
        except Exception as e:
            return {"error": str(e)}

def create_retriever_tool_from_builder(
    force_rebuild: bool = False
) -> LocalRetrieverTool:
    """
    ä»æ•°æ®åº“æ„å»ºå™¨åˆ›å»ºæ£€ç´¢å·¥å…·
    
    Args:
        builder: RAGæ•°æ®åº“æ„å»ºå™¨
        force_rebuild: æ˜¯å¦å¼ºåˆ¶é‡å»ºæ•°æ®åº“
    
    Returns:
        LocalRetrieverTool: æ£€ç´¢å·¥å…·å®ä¾‹
    """
    # æ„å»ºæˆ–åŠ è½½æ•°æ®åº“
    vector_store = build_rag_database(force_rebuild=force_rebuild)
    
    # åˆ›å»ºå·¥å…·
    return LocalRetrieverTool(vector_store)

def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="æœ¬åœ°RAGæ–‡æ¡£æ£€ç´¢ä»£ç†",
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python rag_tool.py "æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒæ–¹æ³•"
  python rag_tool.py "ä»€ä¹ˆæ˜¯DynaSaur" --force-rebuild
  python rag_tool.py "æ·±åº¦å­¦ä¹ ä¼˜åŒ–ç®—æ³•" --max-steps 30 --verbosity 1
  python rag_tool.py "AIå‘å±•è¶‹åŠ¿" --model-id gpt-4o --max-steps 15
        """,
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument(
        "query",
        type=str,
        help="è¦æœç´¢çš„æŸ¥è¯¢å†…å®¹"
    )
    parser.add_argument(
        "--force-rebuild",
        action="store_true",
        help="å¼ºåˆ¶é‡å»ºå‘é‡æ•°æ®åº“"
    )
    parser.add_argument(
        "--max-steps",
        type=int,
        default=20,
        help="ä»£ç†æœ€å¤§æ‰§è¡Œæ­¥æ•° (é»˜è®¤: 20)"
    )
    parser.add_argument(
        "--verbosity",
        type=int,
        default=2,
        help="æ—¥å¿—è¯¦ç»†ç¨‹åº¦ (0-2, é»˜è®¤: 2)"
    )
    parser.add_argument(
        "--planning-interval",
        type=int,
        default=4,
        help="è§„åˆ’é—´éš”æ­¥æ•° (é»˜è®¤: 4)"
    )
    parser.add_argument(
        "--model-id",
        type=str,
        default="o3",
        help="LLMæ¨¡å‹ID (é»˜è®¤: o3ï¼Œå°†è‡ªåŠ¨æ·»åŠ  litellm_proxy/ å‰ç¼€)"
    )
    
    return parser.parse_args()

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_arguments()
    
    print(f"ğŸ” æ­£åœ¨å¤„ç†æŸ¥è¯¢: {args.query}")
    print(f"ğŸ“Š å¼ºåˆ¶é‡å»ºæ•°æ®åº“: {args.force_rebuild}")
    print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: litellm_proxy/{args.model_id}")
    
    # åˆ›å»ºæ£€ç´¢å·¥å…·
    retriever_tool = create_retriever_tool_from_builder(force_rebuild=args.force_rebuild)
    
    # é…ç½®æ¨¡å‹å‚æ•°
    model_params = {
        "model_id": f"litellm_proxy/{args.model_id}",
        "max_completion_tokens": 8192,
        "api_key": os.getenv("API_KEY"),
        "base_url": os.getenv("BASE_URL")
    }
    model = LiteLLMModel(**model_params)

    # åˆ›å»ºä»£ç†
    agent = CodeAgent(
        tools=[retriever_tool],
        model=model,
        max_steps=args.max_steps,
        verbosity_level=args.verbosity,
        planning_interval=args.planning_interval,
        name="local_rag_agent",
        description="""
        ä¸€ä¸ªæœ¬åœ°æ–‡æ¡£æ£€ç´¢ä»£ç†ï¼Œå¯ä»¥æœç´¢æœ¬åœ°æ–‡æ¡£åº“ä¸­çš„ç›¸å…³ä¿¡æ¯ã€‚
        è¿™ä¸ªä»£ç†å¯ä»¥æœç´¢æœ¬åœ°çš„txtã€pdfã€docxã€mdç­‰æ ¼å¼çš„æ–‡æ¡£ã€‚
        æ”¯æŒä¸­æ–‡å’Œè‹±æ–‡æ–‡æ¡£æ£€ç´¢ã€‚
        """,
    )
    
    # è¿è¡Œä»£ç†å¤„ç†æŸ¥è¯¢
    print("ğŸš€ å¼€å§‹æ‰§è¡ŒæŸ¥è¯¢...")
    result = agent.run(args.query)
    print(f"âœ… æŸ¥è¯¢å®Œæˆ!")
    print(f"ğŸ“‹ ç»“æœ: {result}")