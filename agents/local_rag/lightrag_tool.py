import os
import sys
import asyncio
import numpy as np
from pathlib import Path
from typing import List, Optional, Dict, Any

import torch
from transformers import AutoTokenizer, AutoModel
from sentence_transformers import CrossEncoder

from lightrag import LightRAG, QueryParam
from lightrag.utils import EmbeddingFunc, setup_logger
from lightrag.kg.shared_storage import initialize_pipeline_status
from lightrag.llm.openai import openai_complete_if_cache
from lightrag.llm.hf import hf_embed

# ä¸´æ—¶ä¿®æ”¹ sys.argv ä»¥é¿å…ä¸ lightrag.api çš„ argparse å†²çª
# lightrag.api.config åœ¨å¯¼å…¥æ—¶ä¼šè°ƒç”¨ parse_args()
_original_argv = sys.argv.copy()
sys.argv = [sys.argv[0]]  # åªä¿ç•™è„šæœ¬åï¼Œæ¸…ç©ºå…¶ä»–å‚æ•°
from lightrag.api.routers.document_routes import pipeline_index_files
sys.argv = _original_argv  # æ¢å¤åŸå§‹å‚æ•°

from smolagents import Tool

from dotenv import load_dotenv
load_dotenv(override=True)

# ========== é…ç½® ==========
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "Qwen/Qwen3-Embedding-0.6B")
RERANKER_MODEL = os.getenv("RERANKER_MODEL", "Qwen/Qwen3-Reranker-0.6B")
LLM_MODEL = os.getenv("LLM_MODEL", "gpt-5-chat")
LLM_BASE_URL = os.getenv("BASE_URL", "http://localhost:dummy/v1")
LLM_API_KEY = os.getenv("API_KEY", "dummy")
RAG_TOP_K = int(os.getenv("RAG_TOP_K", "10"))
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# ========== å…¨å±€æ¨¡å‹ç¼“å­˜ ==========
import threading

_model_lock = threading.Lock()  # ä»…ç”¨äºæ¨¡å‹åŠ è½½æ—¶çš„å•ä¾‹ä¿æŠ¤
_embedding_model = None
_embedding_tokenizer = None
_reranker_model = None


def load_embedding_model():
    """åŠ è½½ Embedding æ¨¡å‹ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
    global _embedding_model, _embedding_tokenizer
    
    if _embedding_model is not None:
        return _embedding_model, _embedding_tokenizer
    
    with _model_lock:
        if _embedding_model is None:
            print(f"ğŸ”§ åŠ è½½ Embedding æ¨¡å‹: {EMBEDDING_MODEL}")
            _embedding_tokenizer = AutoTokenizer.from_pretrained(
                EMBEDDING_MODEL, trust_remote_code=True
            )
            _embedding_model = AutoModel.from_pretrained(
                EMBEDDING_MODEL, trust_remote_code=True
            ).to(DEVICE)
            _embedding_model.eval()
            print(f"âœ… Embedding æ¨¡å‹åŠ è½½å®Œæˆï¼Œè®¾å¤‡: {DEVICE}")
    
    return _embedding_model, _embedding_tokenizer


def load_reranker_model():
    """
    åŠ è½½ Reranker æ¨¡å‹ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
    """
    global _reranker_model
    
    if _reranker_model is not None:
        return _reranker_model
    
    with _model_lock:
        if _reranker_model is None:
            print(f"ğŸ”§ åŠ è½½ Reranker æ¨¡å‹: {RERANKER_MODEL}")
            _reranker_model = CrossEncoder(RERANKER_MODEL, device=DEVICE)
            
            print(f"âœ… Reranker åŠ è½½å®Œæˆï¼Œè®¾å¤‡: {DEVICE}")
    
    return _reranker_model


async def custom_embedding_func(texts: List[str]) -> np.ndarray:
    """
    è‡ªå®šä¹‰åµŒå…¥å‡½æ•°ï¼ˆä½¿ç”¨ LightRAG çš„ hf_embed + å¢å¼ºï¼‰
    
    Args:
        texts: è¦åµŒå…¥çš„æ–‡æœ¬åˆ—è¡¨
        
    Returns:
        åµŒå…¥å‘é‡æ•°ç»„ï¼Œå½¢çŠ¶ä¸º (len(texts), embedding_dim)
    """
    model, tokenizer = load_embedding_model()
    
    # ä½¿ç”¨ LightRAG çš„ hf_embed è¿›è¡Œæ¨ç†
    embeddings = await hf_embed(texts, tokenizer, model)
    
    # æ·»åŠ  L2 å½’ä¸€åŒ–ï¼ˆå¯¹ä½™å¼¦ç›¸ä¼¼åº¦å¾ˆé‡è¦ï¼‰
    embeddings_tensor = torch.from_numpy(embeddings)
    embeddings_tensor = torch.nn.functional.normalize(embeddings_tensor, p=2, dim=1)
    
    return embeddings_tensor.numpy()


async def custom_rerank_func(
    query: str,
    documents: List[str],
    top_n: Optional[int] = None,
    **kwargs
) -> List[Dict[str, Any]]:
    """
    Qwen3 é‡æ’åºå‡½æ•°ï¼ˆé€ä¸ªå¤„ç†é¿å… padding é—®é¢˜ï¼‰
    
    Args:
        query: æŸ¥è¯¢æ–‡æœ¬
        documents: æ–‡æ¡£åˆ—è¡¨
        top_n: è¿”å›å‰ N ä¸ªç»“æœ
        
    Returns:
        é‡æ’åºç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« index å’Œ relevance_score
    """
    if not documents:
        return []
    
    model = load_reranker_model()
    
    try:
        # é€ä¸ªå¤„ç†æ–‡æ¡£ä»¥é¿å…æ‰¹é‡å¤„ç†çš„ padding é—®é¢˜
        rerank_scores = []
        
        for i, doc in enumerate(documents):
            try:
                query_doc_pair = [query, doc]
                
                score = model.predict([query_doc_pair], show_progress_bar=False)[0]
                rerank_scores.append(float(score))
                
            except Exception as e:
                print(f"âš ï¸ æ–‡æ¡£ {i} rerank å¤±è´¥: {e}")
                rerank_scores.append(0.0)
        
        results = [
            {"index": i, "relevance_score": score}
            for i, score in enumerate(rerank_scores)
        ]
        
        results.sort(key=lambda x: x["relevance_score"], reverse=True)
        
        if top_n is not None and top_n > 0:
            results = results[:top_n]
            
        return results
        
    except Exception as e:
        print(f"âš ï¸ Reranker æ•´ä½“å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return [{"index": i, "relevance_score": 0.0} for i in range(len(documents))]


async def llm_model_func(
    prompt: str,
    system_prompt: Optional[str] = None,
    history_messages: List = [],
    **kwargs
) -> str:
    """
    LLM å‡½æ•°ï¼Œä½¿ç”¨ OpenAI å…¼å®¹ API
    
    Args:
        prompt: ç”¨æˆ·æç¤º
        system_prompt: ç³»ç»Ÿæç¤º
        history_messages: å†å²æ¶ˆæ¯
        
    Returns:
        LLM ç”Ÿæˆçš„å“åº”
    """
    return await openai_complete_if_cache(
        LLM_MODEL,
        prompt,
        system_prompt=system_prompt,
        history_messages=history_messages,
        api_key=LLM_API_KEY,
        base_url=LLM_BASE_URL,
        **kwargs,
    )


class LightRAGManager:
    """
    LightRAG ç®¡ç†å™¨ï¼Œè´Ÿè´£ RAG çš„ç”Ÿå‘½å‘¨æœŸç®¡ç†
    
    èŒè´£ï¼š
    - åˆå§‹åŒ–å’ŒåŠ è½½ LightRAG å®ä¾‹
    - æ„å»ºæ–‡æ¡£ç´¢å¼•
    - æ‰§è¡ŒæŸ¥è¯¢
    - ç®¡ç†æ•°æ®åº“çŠ¶æ€
    """
    
    def __init__(
        self,
        working_dir: str = "./lightrag_storage",
        embedding_dim: Optional[int] = None,
    ):
        """
        åˆå§‹åŒ– RAG ç®¡ç†å™¨
        
        Args:
            working_dir: LightRAG å·¥ä½œç›®å½•
            embedding_dim: åµŒå…¥ç»´åº¦ï¼ˆå¦‚æœä¸º Noneï¼Œä¼šè‡ªåŠ¨æ£€æµ‹ï¼‰
        """
        self.working_dir = working_dir
        self.embedding_dim = embedding_dim
        self.rag: Optional[LightRAG] = None
        self._initialized = False
        
        setup_logger("lightrag")
    
    async def initialize(self) -> None:
        if self._initialized:
            return
        
        print(f"ğŸ”§ åˆå§‹åŒ– LightRAGï¼Œå·¥ä½œç›®å½•: {self.working_dir}")
        
        os.makedirs(self.working_dir, exist_ok=True)
        
        if self.embedding_dim is None:
            print("ğŸ” æ£€æµ‹ Embedding ç»´åº¦...")
            test_embedding = await custom_embedding_func(["test"])
            self.embedding_dim = test_embedding.shape[1]
            print(f"ğŸ“ Embedding ç»´åº¦: {self.embedding_dim}")
        
        self.rag = LightRAG(
            working_dir=self.working_dir,
            llm_model_func=llm_model_func,
            embedding_func=EmbeddingFunc(
                embedding_dim=self.embedding_dim,
                func=custom_embedding_func,
            ),
            rerank_model_func=custom_rerank_func,
        )
        
        await self.rag.initialize_storages()
        await initialize_pipeline_status()
        
        self._initialized = True
        print("âœ… LightRAG åˆå§‹åŒ–å®Œæˆ")
    
    def is_database_built(self) -> bool:
        """
        æ£€æŸ¥æ•°æ®åº“æ˜¯å¦å·²æ„å»º
        
        Returns:
            å¦‚æœæ•°æ®åº“å­˜åœ¨ä¸”éç©ºï¼Œè¿”å› True
        """
        if not os.path.exists(self.working_dir):
            return False
        
        required_files = [
            "kv_store_full_docs.json",  # æ–‡æ¡£å­˜å‚¨
            "vdb_chunks.json",          # å‘é‡æ•°æ®åº“
        ]
        
        for filename in required_files:
            file_path = os.path.join(self.working_dir, filename)
            if not os.path.exists(file_path):
                return False
            if os.path.getsize(file_path) == 0:
                return False
        
        return True
    
    async def build_from_directory(
        self,
        documents_path: str,
        force_rebuild: bool = False
    ) -> int:
        """
        ä»ç›®å½•æ„å»ºæ–‡æ¡£ç´¢å¼•
        
        Args:
            documents_path: æ–‡æ¡£ç›®å½•è·¯å¾„
            force_rebuild: æ˜¯å¦å¼ºåˆ¶é‡å»º
            
        Returns:
            æˆåŠŸå¤„ç†çš„æ–‡ä»¶æ•°é‡
        """
        if not self._initialized:
            await self.initialize()
        
        if not force_rebuild and self.is_database_built():
            print("ğŸ“š æ•°æ®åº“å·²å­˜åœ¨ï¼Œè·³è¿‡æ„å»ºï¼ˆä½¿ç”¨ force_rebuild=True å¼ºåˆ¶é‡å»ºï¼‰")
            return 0
        
        directory_path = Path(documents_path)
        file_paths = []
        for file_path in directory_path.rglob("*"):
            if file_path.is_file():
                file_paths.append(file_path)
        
        if not file_paths:
            print(f"âš ï¸ åœ¨ {documents_path} ä¸­æœªæ‰¾åˆ°æ–‡ä»¶")
            return 0
        
        print(f"ğŸ“Š æ‰¾åˆ° {len(file_paths)} ä¸ªæ–‡ä»¶ï¼Œå¼€å§‹ç´¢å¼•...")
        
        try:
            await pipeline_index_files(self.rag, file_paths)
            print(f"âœ… æ–‡ä»¶ç´¢å¼•å®Œæˆï¼ˆæŸ¥çœ‹æ—¥å¿—äº†è§£å¤„ç†è¯¦æƒ…ï¼‰")
            return len(file_paths)
        except Exception as e:
            print(f"âŒ ç´¢å¼•æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return 0
    
    async def query(
        self,
        query: str,
        mode: str = "mix",
        top_k: int = 5,
        use_rerank: bool = False
    ) -> str:
        """
        æ‰§è¡ŒæŸ¥è¯¢
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            mode: æ£€ç´¢æ¨¡å¼ (naive/local/global/hybrid/mix)
            top_k: è¿”å›ç»“æœæ•°é‡
            use_rerank: æ˜¯å¦ä½¿ç”¨é‡æ’åº
            
        Returns:
            æŸ¥è¯¢ç»“æœ
        """
        if not self._initialized:
            await self.initialize()
        
        if not self.is_database_built():
            return "âŒ æ•°æ®åº“æœªæ„å»ºã€‚è¯·å…ˆæ„å»ºæ–‡æ¡£ç´¢å¼•ã€‚"
        
        result = await self.rag.aquery(
            query,
            param=QueryParam(
                mode=mode,
                top_k=top_k,
                chunk_top_k=top_k * 2,
                enable_rerank=use_rerank,
            )
        )
        
        return result
    
    def get_stats(self) -> Dict[str, Any]:
        """
        è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        stats = {
            "working_dir": self.working_dir,
            "embedding_dim": self.embedding_dim,
            "embedding_model": EMBEDDING_MODEL,
            "reranker_model": RERANKER_MODEL,
            "llm_model": LLM_MODEL,
            "device": DEVICE,
            "database_built": self.is_database_built(),
            "initialized": self._initialized,
        }
        
        return stats


class LightRAGRetrieverTool(Tool):
    """
    åŸºäº LightRAG çš„æ£€ç´¢å·¥å…·
    """
    
    name = "lightrag_retriever"
    description = """åŸºäºçŸ¥è¯†å›¾è°±çš„è¯­ä¹‰æ£€ç´¢å·¥å…·ï¼Œä»æœ¬åœ°æ–‡æ¡£çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯ã€‚"""
    
    inputs = {
        "query": {
            "type": "string",
            "description": "è¦æœç´¢çš„å†…å®¹ï¼Œä¸è¦ä½¿ç”¨ç®€å•çš„å…³é”®è¯ï¼ŒæŸ¥è¯¢å°½é‡å…·ä½“ä¸”ç”¨æ„æ˜ç¡®ã€‚"
        }
    }
    output_type = "string"
    
    def __init__(
        self,
        working_dir: str = "./lightrag_storage",
        **kwargs
    ):
        """
        åˆå§‹åŒ– LightRAG æ£€ç´¢å·¥å…·
        
        Args:
            working_dir: LightRAG å·¥ä½œç›®å½•ï¼ˆæ•°æ®åº“å­˜å‚¨ä½ç½®ï¼‰
        """
        super().__init__(**kwargs)
        
        self.working_dir = working_dir
        
        self.rag_manager = LightRAGManager(working_dir=working_dir)
        
        try:
            self._run_async_init()
            
            if self.rag_manager.is_database_built():
                print("âœ… LightRAG æ•°æ®åº“åŠ è½½æˆåŠŸ")
                stats = self.rag_manager.get_stats()
                print(f"   - å·¥ä½œç›®å½•: {stats['working_dir']}")
                print(f"   - åµŒå…¥ç»´åº¦: {stats['embedding_dim']}")
                print(f"   - è®¾å¤‡: {stats['device']}")
            else:
                print("âš ï¸ æœªæ‰¾åˆ° LightRAG æ•°æ®åº“")
                print(f"   å·¥ä½œç›®å½•: {self.working_dir}")
            
        except Exception as e:
            print(f"âš ï¸ åˆå§‹åŒ– LightRAG æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
    
    def _run_async_init(self):
        """
        è¿è¡Œå¼‚æ­¥åˆå§‹åŒ–
        """
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(
                        lambda: asyncio.run(self.rag_manager.initialize())
                    )
                    future.result(timeout=60.0)
            else:
                loop.run_until_complete(self.rag_manager.initialize())
        except RuntimeError:
            asyncio.run(self.rag_manager.initialize())
    
    def forward(
        self,
        query: str
    ) -> str:
        """
        ä»æ–‡æ¡£åº“ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯
        
        Args:
            query: æŸ¥è¯¢å†…å®¹
            
        Returns:
            åŸºäºæ–‡æ¡£å†…å®¹ç”Ÿæˆçš„ç»¼åˆç­”æ¡ˆ
        """

        mode = "mix"
        top_k = RAG_TOP_K
        use_rerank = True
        try:
            if not self.rag_manager.is_database_built():
                return "æœªæ‰¾åˆ°ç›¸å…³å†…å®¹ã€‚æ•°æ®åº“å°šæœªæ„å»ºã€‚"
            
            result = self._run_async_query(query, mode, top_k, use_rerank)
            return result
            
        except Exception as e:
            error_msg = f"æ£€ç´¢è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}"
            return error_msg
    
    def _run_async_query(
        self,
        query: str,
        mode: str,
        top_k: int,
        use_rerank: bool
    ) -> str:
        """
        è¿è¡Œå¼‚æ­¥æŸ¥è¯¢
        """
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(
                        lambda: asyncio.run(self.rag_manager.query(query, mode, top_k, use_rerank))
                    )
                    return future.result()
            else:
                return loop.run_until_complete(
                    self.rag_manager.query(query, mode, top_k, use_rerank)
                )
        except RuntimeError:
            return asyncio.run(self.rag_manager.query(query, mode, top_k, use_rerank))
    
    
    def get_stats(self) -> Dict[str, Any]:
        return self.rag_manager.get_stats()
    

def example_with_smolagents():
    """ä¸ smolagents é›†æˆä½¿ç”¨ç¤ºä¾‹"""
    print("\n" + "=" * 80)
    print("ğŸ“– LightRAG + Smolagents é›†æˆç¤ºä¾‹")
    print("=" * 80 + "\n")
    
    from smolagents import CodeAgent, LiteLLMModel
    
    tool = LightRAGRetrieverTool(working_dir="./lightrag_storage")
    
    model = LiteLLMModel(
        model_id=f"litellm_proxy/{LLM_MODEL}",
        api_key=os.getenv("API_KEY"),
        base_url=os.getenv("BASE_URL")
    )
    
    agent = CodeAgent(
        tools=[tool],
        model=model,
        max_steps=20,
        verbosity_level=2,
    )
    
    query = "ä½¿ç”¨æ–‡æ¡£æ£€ç´¢å·¥å…·ï¼Œæ€»ç»“DynaSaurç›¸å…³å†…å®¹"
    print(f"ğŸ¤– Agent æŸ¥è¯¢: {query}")
    print("-" * 80)
    result = agent.run(query)
    print(f"\nâœ… Agent ç»“æœ:\n{result}")


async def example_manual_workflow():
    """æ‰‹åŠ¨å·¥ä½œæµç¤ºä¾‹ï¼ˆæ›´çµæ´»çš„æ§åˆ¶ï¼‰"""
    print("\n" + "=" * 80)
    print("ğŸ“– LightRAG æ‰‹åŠ¨å·¥ä½œæµç¤ºä¾‹")
    print("=" * 80 + "\n")
    
    print("æ­¥éª¤1: åˆ›å»º RAG ç®¡ç†å™¨")
    manager = LightRAGManager(working_dir="./lightrag_storage")
    await manager.initialize()
    
    print("\næ­¥éª¤2: æ£€æŸ¥æ•°æ®åº“çŠ¶æ€")
    if not manager.is_database_built():
        print("   æ•°æ®åº“æœªæ„å»ºï¼Œå¼€å§‹æ„å»º...")
        file_count = await manager.build_from_directory("./my_documents")
        print(f"   å·²ç´¢å¼• {file_count} ä¸ªæ–‡ä»¶")
    else:
        print("   æ•°æ®åº“å·²å­˜åœ¨")
    
    print("\næ­¥éª¤3: æ‰§è¡ŒæŸ¥è¯¢")
    result = await manager.query(
        query="ä»€ä¹ˆæ˜¯DynaSaur",
        mode="mix",
        top_k=RAG_TOP_K,
        use_rerank=True
    )
    print(f"   æŸ¥è¯¢ç»“æœ:\n{result}")
    
    print("\næ­¥éª¤4: æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯")
    stats = manager.get_stats()
    for key, value in stats.items():
        print(f"   {key}: {value}")

def main():
    """æµ‹è¯•å‡½æ•°"""
    if len(sys.argv) > 1:
        mode = sys.argv[1]
        if mode == "--agent":
            example_with_smolagents()
        elif mode == "--manual":
            asyncio.run(example_manual_workflow())
        else:
            print(f"âŒ æœªçŸ¥æ¨¡å¼: {mode}")
            print("\nå¯ç”¨æ¨¡å¼:")
            print("  (æ— å‚æ•°)  - ç‹¬ç«‹ä½¿ç”¨ç¤ºä¾‹")
            print("  --agent   - ä¸ smolagents é›†æˆç¤ºä¾‹")
            print("  --manual  - æ‰‹åŠ¨å·¥ä½œæµç¤ºä¾‹")

if __name__ == "__main__":
    main()
