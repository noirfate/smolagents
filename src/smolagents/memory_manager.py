"""
è®°å¿†ç®¡ç†å™¨ - åŸºäºPlanningå‘¨æœŸçš„æ™ºèƒ½è®°å¿†å‹ç¼©
åˆ©ç”¨smolagentsç°æœ‰çš„planningæœºåˆ¶ä½œä¸ºè®°å¿†åˆ†å‰²ç‚¹

å‹ç¼©ç­–ç•¥ï¼š
- å½“æœ‰è‡³å°‘2ä¸ªplanning stepæ—¶ï¼Œä¿ç•™æœ€è¿‘çš„{action, plan}ç»„åˆ
- å‹ç¼©å€’æ•°ç¬¬äºŒä¸ªplanning stepä¹‹å‰çš„æ‰€æœ‰å†å²
- ç»“æ„ï¼šsystem_prompt + compressed_history + recent_{action, plan}
"""

from typing import List
from .models import ChatMessage, MessageRole
from .memory import MemoryStep, ActionStep, PlanningStep

__all__ = [
    "MemoryManager"
]

class MemoryManager:
    """è½»é‡çº§è®°å¿†ç®¡ç†å™¨ï¼ŒåŸºäºPlanningå‘¨æœŸè¿›è¡Œè®°å¿†å‹ç¼©"""
    
    def __init__(self):
        self._compressed_history = None  # ç¼“å­˜å‹ç¼©ç»“æœ
        self._last_compressed_index = -1  # è®°å½•ä¸Šæ¬¡å‹ç¼©çš„ä½ç½®
    
    def get_planning_step_indices(self, memory_steps: List[MemoryStep]) -> List[int]:
        """ä¸€æ¬¡éå†è·å–æ‰€æœ‰PlanningStepçš„ä½ç½®"""
        return [i for i, step in enumerate(memory_steps) if isinstance(step, PlanningStep)]
    
    def get_compression_split_point(self, memory_steps: List[MemoryStep]) -> int:
        """è·å–å‹ç¼©åˆ†å‰²ç‚¹ï¼šç›´æ¥ä½¿ç”¨å€’æ•°ç¬¬äºŒä¸ªplanning stepçš„ä½ç½®
        
        è¿”å›åˆ†å‰²ç‚¹ç´¢å¼•ï¼Œå‹ç¼©[0:split_point]ï¼Œä¿ç•™[split_point:]
        ä¿ç•™éƒ¨åˆ†ä»å®Œæ•´çš„planning stepå¼€å§‹ï¼ŒåŒ…å«æœ€è¿‘çš„å®Œæ•´è§„åˆ’å‘¨æœŸ
        
        è¾¹ç•Œæƒ…å†µå¤„ç†ï¼š
        - å¦‚æœå€’æ•°ç¬¬äºŒä¸ªplanning stepä¹‹å‰æ²¡æœ‰ActionStepï¼Œä¸è¿›è¡Œå‹ç¼©
        - ç¡®ä¿å‹ç¼©çš„éƒ¨åˆ†åŒ…å«å®Œæ•´çš„{action, plan}ç»„åˆ
        """
        planning_indices = self.get_planning_step_indices(memory_steps)
        
        # è‡³å°‘éœ€è¦ä¸¤ä¸ªplanning stepæ‰èƒ½æ‰¾åˆ°å€’æ•°ç¬¬äºŒä¸ª
        if len(planning_indices) < 2:
            return -1
        
        second_last_plan_idx = planning_indices[-2]
        
        # è¾¹ç•Œæƒ…å†µï¼šæ£€æŸ¥å€’æ•°ç¬¬äºŒä¸ªplanning stepä¹‹å‰æ˜¯å¦æœ‰ActionStep
        # å¦‚æœæ²¡æœ‰ï¼Œè¯´æ˜æ²¡æœ‰å®Œæ•´çš„{action, plan}ç»„åˆå¯ä»¥å‹ç¼©
        has_action_before_second_last_plan = any(
            isinstance(step, ActionStep) for step in memory_steps[:second_last_plan_idx]
        )
        
        if not has_action_before_second_last_plan:
            return -1  # ä¸è¿›è¡Œå‹ç¼©
        
        return second_last_plan_idx
    
    def compress_historical_steps(self, historical_steps: List[MemoryStep], model) -> str:
        """å°†å†å²æ­¥éª¤å‹ç¼©ä¸ºç»“æ„åŒ–æ€»ç»“ï¼Œå¤±è´¥æ—¶è¿”å›None"""
        if not historical_steps:
            return None
        
        # æ„å»ºå®Œæ•´çš„å†å²æ‰§è¡Œä¿¡æ¯
        execution_log = []
        
        for i, step in enumerate(historical_steps, 1):
            if isinstance(step, ActionStep):
                step_info = f"**Step {i}** (ActionStep):\n"
                
                # å®Œæ•´çš„å·¥å…·è°ƒç”¨ä¿¡æ¯
                if step.tool_calls:
                    step_info += "  Tool calls:\n"
                    for tc in step.tool_calls:
                        step_info += f"    - {tc.name}({tc.arguments})\n"
                
                # å®Œæ•´çš„è§‚å¯Ÿç»“æœ
                if step.observations:
                    step_info += f"  Observations: {step.observations}\n"
                
                # é”™è¯¯ä¿¡æ¯
                if step.error:
                    step_info += f"  âš ï¸ Error: {step.error}\n"
                
                execution_log.append(step_info)
            
            elif isinstance(step, PlanningStep):
                step_info = f"**Step {i}** (PlanningStep):\n"
                step_info += f"  Plan: {step.plan}\n"
                execution_log.append(step_info)
        
        full_log = "\n".join(execution_log)
        
        # ä¼˜åŒ–çš„å‹ç¼©æç¤ºï¼Œç»“åˆsmolagentsè®¾è®¡ç†å¿µ
        prompt = f"""You are a memory compression assistant for an intelligent agent. Please compress the following execution history into a structured summary.

**Smolagents Design Philosophy**:
- Tool-based problem solving
- Planning â†’ Execution â†’ Observation â†’ Learning cycles
- Multi-step reasoning and tool collaboration
- Learning from errors and adjusting strategies

**Execution history to compress**:
{full_log}

**Compression requirements**:
Please generate a structured execution summary with the following sections:

1. **Solution Path Overview**: Brief description of the overall problem-solving approach and strategy
2. **Key Tool Usage**: List the main tools used and their roles (preserve logical relationships between tool calls)
3. **Important Discoveries**: Summarize key information, data, or conclusions obtained
4. **Strategy Adjustments**: Record mental shifts and method changes when encountering problems
5. **Unresolved Issues**: List problems or questions that still need to be addressed
6. **Experience Gained**: Lessons learned from both successes and failures

**Format requirements**:
- Maintain accuracy and completeness of information
- Highlight the logical chain of tool usage
- Preserve important data and conclusions
- Be concise but retain key details
- Make it useful for subsequent reasoning

Please begin the compression summary:"""

        try:
            messages = [ChatMessage(role=MessageRole.USER, content=prompt)]
            response = model.generate(messages)
            
            if response.content:
                compressed = f"ğŸ“‹ **Execution History Summary** (compressed {len(historical_steps)} steps):\n\n{response.content}"
                return compressed
            else:
                # æ¨¡å‹è¿”å›ç©ºå†…å®¹ï¼Œè§†ä¸ºå‹ç¼©å¤±è´¥
                print(f"âš ï¸ è®°å¿†å‹ç¼©å¤±è´¥: æ¨¡å‹è¿”å›ç©ºå†…å®¹")
                return None
            
        except Exception as e:
            print(f"âš ï¸ è®°å¿†å‹ç¼©å¤±è´¥: {e}")
            return None  # å‹ç¼©å¤±è´¥æ—¶è¿”å›Noneï¼Œè®©ä¸Šå±‚æ–¹æ³•fallbackåˆ°åŸå§‹é€»è¾‘
    
    def write_memory_to_messages_with_compression(self, agent, summary_mode: bool = False) -> List[ChatMessage]:
        """å¸¦å‹ç¼©åŠŸèƒ½çš„è®°å¿†è½¬æ¢æ–¹æ³•"""
        memory_steps = agent.memory.steps
        
        split_point = self.get_compression_split_point(memory_steps)
        
        # å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„åˆ†å‰²ç‚¹ï¼Œä½¿ç”¨åŸå§‹æ–¹æ³•
        if split_point < 0:
            return agent._original_write_memory_to_messages(summary_mode)
        
        if split_point > self._last_compressed_index:
            # å‹ç¼©åˆ†å‰²ç‚¹ä¹‹å‰çš„å†å²ï¼ˆä¿ç•™æœ€è¿‘çš„{action, plan}ç»„åˆï¼‰
            historical_steps = memory_steps[:split_point]
            compressed_result = self.compress_historical_steps(historical_steps, agent.model)
            
            # å¦‚æœå‹ç¼©å¤±è´¥ï¼Œfallbackåˆ°åŸå§‹æ–¹æ³•
            if compressed_result is None:
                print("âš ï¸ è®°å¿†å‹ç¼©å¤±è´¥ï¼Œå›é€€åˆ°åŸå§‹æ–¹æ³•ä»¥ä¿è¯å†å²è®°å½•å®Œæ•´æ€§")
                return agent._original_write_memory_to_messages(summary_mode)
            
            self._compressed_history = compressed_result
            self._last_compressed_index = split_point
        
        # æ„å»ºæ¶ˆæ¯
        messages = []
        
        # 1. System prompt
        messages.extend(agent.memory.system_prompt.to_messages(summary_mode))
        
        # 2. å‹ç¼©çš„å†å²æ‘˜è¦
        if self._compressed_history:
            messages.append(ChatMessage(
                role=MessageRole.ASSISTANT,
                content=[{"type": "text", "text": self._compressed_history}]
            ))
        
        # 3. æœ€è¿‘çš„{action, plan}ç»„åˆåŠåç»­è®°å½•
        recent_steps = memory_steps[split_point:]
        for step in recent_steps:
            messages.extend(step.to_messages(summary_mode))
        
        return messages